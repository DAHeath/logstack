
\section{Technical Overview of Our Approach}
\label{sec:techOverview}

We  now review in our main contributions with sufficient  detail to present most interesting  technical challenges and  solutions.

\subsection{$O(b\log b)$ Stacked Garbling}
\label{sec:techOverviewSG}

Our main contribution is reducing the computation associated with branch processing in SGC from $O(b^2)$ to $O(b \log b)$.  Our constants are low, and together \G and \E make $\approx 3 b \log b$ calls to \Gb\ and $b\log b$ calls  to \Ev.  \vlad{to confirm exact numbers.}

We now continue the discussion of~\Cref{sec:intuition} and add  technical detail.  
Our main task is the garbage collection of output labels of {\em incorrectly guessed} (\truth,\guess) combinations 
%w.r.t. taken (active) branch, 
where
\guess is \E's guess of the active branch, and \truth defines the active branch w.r.t. \guess.   Wlog, let $b$ be a power of $2$ to simplify notation.

Fix one of $b$ choices for \guess.  In contrast with previous work~\HK, which then considers $b$ choices for \truth independently from \guess, we define \truth\ {\em in relation} to \guess, and consider fewer \truth options.  Namely,
% for each tree level $i$, 
we  let \truth  denote in which sibling subtree of \guess the executed branch resides (cf. discussion in~\Cref{sec:intuition}).  There are $\log b$ choices for this \truth.

Consider an illustrative example of~\Cref{fig:seed-tree}, where $\guess=0$.  Then any of active branches $C_4-C_7$ will correspond to $\truth = 1$ as they all belong to the level-$1$ sibling subtree of $C_0$.  We want \E's unstacking and evaluation to result in identical garbage output labels across all cases when active branch is one of $C_4-C_7$  (and similarly across each level-$i$ subtree).  To achieve this, \E will use garbled labels corresponding to the ID of the active branch \aid.

% We achieve this by 

% (the choice of $\truth = 0$ is only valid for the correct guess), one of which corresponds to the correct guess.  If  the guess is correct, active branch resides in none of the $\log b$ sibling subtrees, and we can denote $\truth=0$ in this case. 

%We now show how \G and \E can efficiently process each of the $b\log b$ cases.   

Recall, as part of GC  evaluation, \aid is computed by the circuit, and \E obtains the corresponding $ \log b$-long vector of GC labels $\vec S$, where
%, whose corresponding plaintext values define the id of the active branch. 
 %each label 
$S[i] \in \{S^0[i],S^1[i]\}$; \G of course knows all labels $S^0[i],S^1[i]$.
% he uses the labels as seeds in garbling the branches as follows.

\G  garbles branches  $C_0,...,C_7$ as follows.  \G generates  random seed for the root of the tree (denoted $s_{0-7}$ for the $8$-leaf tree in~\Cref{fig:seed-tree}), and uses it to pseudorandomly derive seeds for each node of the tree.  This is done in the standard manner, e.g. the immediate children of a seed $s$ are the PRF evaluations on inputs $0$ and $1$ with the key $s$.
\G uses leaf seed $s_i$ to garble each branch $\cir_i$ and stacks all garbled branches $\mat=\bigoplus_i \gcir_i$.  

%will take labels $\vec S$ as input, and

\G additionally generates and sends to \E a garbled gadget \gadget that will aid \E in garbled branch reconstructions.  Specifically, \gadget can be applied to any tree node \node; it is constructed to 
  allow  \E to correctly reconstruct the seed $s_\node$ if and only if \node is a {\em sibling root} (i.e.  a root of a sibling subtree) of the leaf \aid.   \gadget will input \aid in the form of the labels $\vec S$ and can easily be implemented as a collection of garbled rows.  Importantly, since this is a fixed gadget, when evaluated on a node \node that is not a sibling root of \aid, \E will obtain a {\em single} predictable to \G\   fixed garbage value. % for each specific leaf $\ell$.

For example in~\Cref{fig:seed-tree}, if active branch is $\aid=4$, then applying the gadget to nodes $\node_{0-3}, \node_{6-7}, \node_{5}$ will allow the reconstruction of correct seeds $s_{0-3}, s_{6-7}, s_{5}$.  Applying \gadget to other nodes (which would under the hood mean decrypting a correspondingly different garbled row) will result in \E obtaining garbage seeds.


\medskip

It is now intuitive how \E will proceed with unstacking.  She will use the gadget and obtain a tree of random-looking seeds; of $2b$ seeds, only $\log b$ seeds just off the path to \aid (corresponding to sibling roots) will be correct.
\E will guess \guess; assuming \guess, she will use (only) the sibling seeds of \guess to derive all the $b-1$ leaf seeds.  She then garbles all branches $\cir_i$ and unstacks the corresponding GCs $\gcir_i$.

Clearly, if $\guess=\aid$, she will reconstruct and unstack the intended garbled circuits, and therefore will obtain the intended circuit for evaluation.  Let's consider the case that her guess is wrong. Firstly, there is no security violation since she will never receive any additional valid seeds; \E will simply unstack wrong branches garbled with the wrong seeds.





Finally, we note  that this argument clearly generalizes to the tree of every size.


{\em Computational cost accounting.} Let's now verify that we indeed reduced the number of calls to \Gb\ and \Ev\ to $O(b \log b)$.  This is easily done by first considering how many such calls are made by \E.  
Consider branch $\cir_i$.  It is garbled $\log b$ times, once with a seed (ultimately) derived from each seed on the path to the root. \vlad{rephrase?}  Thus, total number of calls by \E to \Gb\ is $b \log b$ and to \Ev\ is exactly $b$.  

To construct \gadget, \G must obtain all possible garbage labels.  Hence \G emulates the work done by \E in its entirety  for {\em every} possible value of $\truth\in\{1,...,\log b\}$.  Hence it makes total  $b \log^2 b$ number of calls to \Gb\ and $b \log b$ calls to  \Ev.  
%hence it  makes  $\log b$ calls per branch to \Gb\ and $b\log b$ total calls to \Ev\ to obtain all garbage labels.




















\ignore{



We want to allow \E to deterministically obtain any seed from any of its parents (i.e. nodes on the path to the root).  For example, \E should be able to deterministically obtain any of the seeds $s_4,...,s_7$ from seed $s_{4-7}$, and seeds $s_4, s_5$ -- from seed $s_{4-5}$. 


 Because derivation is deterministic, deriving from another seed in place of $s_{4-7}$, results in 
Clearly, this is easy to achieve, e.g., using a suitable garbled table gadget.








 As in \HK, our \E will make a guess $\guess\in[0..b-1]$ and use these labels as seeds to reconstruct branches guessed as {\em not} taken.  If the guess was correct, then correct garbled branches will be unstacked, and \E will have obtained the correct garbling of the active branch.  Crucially, if the guess was incorrect, we obtain predictable garbage output labels.



By way of example, let's consider the special case of $\guess=0$ and $\truth = 100$ and refer to the illustration of~\Cref{fig:seed-tree} depicting the $b=8$ branches and the binary tree built on it.  It will be clear that this idea naturally generalizes to any \guess.  \E starts processing at the root of the tree.  She made $\guess = 0$, while active branch is $C_4$.  





 Consider at each level of the tree the dichotomy of whether leaves indexed by \guess and \truth are in the same subtree. Our idea is, given a fixed \guess, to  enforce that \E identically processes \guess for {\em all} values of \truth which belong to the same sibling subtree of guess. This narrows \Gâ€™s uncertainty for each value of \guess: each value of \guess has only $\log b$ possible \truth options, one for each sibling subtree of \guess.  In turn, this results in $b\log b$  sets of garbage labels, and $b\log b$ garblings required by \G. \todo{and by \E?}


}


	
	


% At a high level, the circuit generator, i.e. the player who constructs
% the material,





\subsection{Lower bound}
\label{sec:techOverviewLB}






\subsection{Stacked garbling without Random Oracles}
\label{sec:techOverviewRO}




