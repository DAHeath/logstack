\section{Related Work}\label{sec:relwork}

\subsection{When to use \ourscheme: Computational constraints on branching factor}

In this section, we discuss what practical constraints (e.g., wall clock time and implied upper bound on branching factor) are effected by the computational cost of our scheme. 

\paragraph{Assumption: speed of GC generation vs transmission.}  To discuss the best strategy for applying \ourscheme, we must establish relative costs of GC generation and transmission.  Based on our experiments, as well as on~\cite{XiaoPersonalComm}, a commodity laptop running a single core can generate GCs at about $3\times$ the network bandwidth of $1$ Gbps channel.  Engaging  $4$ cores, a typical configuration, would imply up to $12\times$ higher GC generation speed vs transmission.   Desktop CPUs have both higher numbers of cores and per-core processing power.  At the same time, while 1Gbps is a typical speed in the LAN setting, WAN speeds are much lower, e.g. 100Mbps.

In sum, we can conservatively set that computing device is $12\times$ faster than the transmission device.


\medskip

It is easiest to approach this evaluation by establishing the {\em baseline} against which we measure SGC and \ourscheme\ performance.    That is, our baseline is a circuit with conditionals \cir, which we must securely evaluate as-is, and to which we do not apply any program transformations.  We may compare 2PC based on \ourscheme with with Yao GC, both instantiated with base scheme half-gates~\cite{EC:ZahRosEva15}.



\paragraph{Strategy: Always apply \ourscheme.}  With a few caveats described next, using \ourscheme for branching will {\em always} improve over standard GC.  Here is why:  Recall, we are given a circuit \cir which we  evaluate as-is.  Further, recall that GC generation is about $20\times$ faster than its transmission 


\subsection{title}

GC is the most popular and often the fastest approach to secure two-party computation
(2PC).  Until recently, it was believed that it is necessary
to transmit the entire GC during 2PC, even for conditional branches that
are not taken.  Recent breakthrough work~\HK showed that this folklore belief is false, and that  it is enough to only transmit GC material  proportional to the
longest execution path rather than to the entire circuit.

While the protocol of~\HK is concretely efficient, its quadratic computational cost presents a  limitation in settings where branching factor $b$ is beyond $5$ or, perhaps, $10$.  This estimate is based on the fact that on a commodity laptop and 1Gbps network, GC generation (and also evaluation by \E) is only about $20\times$ faster than its network transmission.  In most reasonable  settings and use cases, applying \HK to branching with factors beyond $30$ is not likely to improve total computation time.  Our work pushes this boundary significantly.

Above we only discussed the garbling costs in terms of the number of garbled circuits.  Crucially, memory management is a significant performance factor in GC in general, and in particular in~\HK garbling.  Indeed, retrieving an already-garbled material from RAM may take similar or longer time than regarbling from scratch while operating in cache.

In addition to significantly improving computation cost (i.e. number of calls to \Gb\ and \Ev), our approach offers {\em dramatically} better memory utilization, as we discuss in~\Cref{sec:ourContrib}.  Quadratic construction of~\HK requires keeping track of a quadratic number of garbled circuits \vlad{to check}, while only XXX GCs are in memory at a time.  

As a result, in this work we essentially eliminate the concern of increased computation due to Stacked Garbling for standard settings (1 Gbps network, commodity laptop computing device).   That is, given a circuit

\vlad{be clear: there are two ways to see comp performance. Starting with \cir, are we going to be slower than naive due to computation, and 2) compared to naive eval of netlist of material of same size as stacked material}


{\em Technical difference between our and~\HK binary braching.}
%The multiplexer is \emph{not stacked}.
The careful reader familiar with \cite{EPRINT:HeaKol20b} may be
confused about our key idea: Stacked Garbling first presents a
recursive binary tree approach to branching that they later
discard in favor of a more efficient vector approach.
So why is our binary tree approach better?
The problem with \cite{EPRINT:HeaKol20b}'s recursive construction
is that the evaluator recursively computes the multiplexer for
nested sub-conditionals.
However, doing so leads to a recursive emulation whereby \E
emulates herself (and hence \G emulates himself as well).
This recursion leads to quadratic cost for both players.
The way out is to treat the multiplexer separately, and to opt not
to stack it.
If multiplexers are not stacked, then \E need not compute them.




\ignore{\item The conditional branches are \emph{nested}.
	That is, instead of organizing $b$ branches as a vector, we
	organize them into a binary tree.
	Suppose \E guesses that a particular branch $\cir_i$ is taken,
	while in fact $\cir_j$, a member of a different subtree, is taken.
	The key idea of our approach is that we ensure that when \ev
	garbles the entire subtree containg $\cir_j$ but not $\cir_i$, she
	computes the same material, regardless of $j$.\todo{intuition still unclear}
}
