\section{Introduction}\label{sec:intro}

Secure two party computation (2PC) of programs representable as Boolean circuits can be efficiently achieved using garbled circuits (GC).
%
However,  circuit-based MPC in general is problematic because conditional
control flow does not have an efficient circuit representation:
in the cleartext program, only the taken execution is computed whereas in
the circuit \emph{all} branches must be computed.
%While this creates obvious relative overheads in conditional statements, the most significant impact is on general control flow, such as a sequence of loops.

%
Until recently, it was assumed that the players must not only compute
all branches, but also transmit a string of \emph{material} (e.g., the garbled circuit itself) 
proportional to all branches.  
Since communication is the bottleneck in GC, transmission of this large string was
problematic for programs with conditional behavior.

Stacked Garbling~\HK, which we (and \HK)  interchangeably call Stacked Garbled Circuit (SGC), shows that
this expensive branching-based communication blow-up is unnecessary: the players need only
send enough material for the single longest branch. This single
piece of \emph{stacked} material can be re-used across all conditional branches, thus
dramatically reducing communication cost.
%
This improvement does come with one important downside:
Stacked Garbling requires the players to compute far more than they
would have without stacking.
In particular, the players\todo{both players?} pay $O(b^2)$ computation where $b$ is the
branching factor.

\subsection{Source of $O(b^2)$ Computation Cost of Stacked Garbling }
\label{sec:bsquaredcost}

It is instructive to explain the source of this $O(b^2)$ added computation, to help explain our work.

At a high level, Stacked Garbling requires the evaluator to
``guess'' which branch is taken (she in fact tries  all $b$
branches) and evaluate this guessed branch.
When she guesses right, she simply evaluates the taken branch and
computes valid GC output labels for that branch.
However, when she guesses wrong, she ends up evaluating the branch
with so-called garbage material (material that is a random-looking string\todo{isn't material correct, we just evaluate on (fixed) garbage input values and on the wrong plaintext circuit?}, not
an encryption of circuit truth tables), and ends up computing
\emph{garbage output labels} (i.e., labels that are not the encryption
of $0$ or $1$, but are instead random-looking strings).
%
To proceed past the exit of the conditional and continue garbled evaluation, it is necessary to
`collect'  these garbage labels, i.e., obliviously  discard them in favor of the valid
labels.  (Of course, the final output labels of the conditional will be fresh,  so that they cannot be cross-referenced with those obtained in branch evaluation.)


\HK show how to collect this garbage without
interaction.  They do this by requiring  SGC Generator \G to construct a
garbled gadget called a multiplexer.
Building the \HK multiplexer requires \G to {\em know all possible
garbage labels} the evaluator might obtain.  Once this is satisfied, it is easy to see how \G can produce a gadget (e.g., an appropriately wired set of garbled translation tables) eliminating garbage and propagating true values.


{\bf The Uncertainty of \G.}
It is possible for generator to acquire these garbage labels.  \HK achieve this by having \G  emulate the actions of the
evaluator \E on all non-taken branches.  To see how this can be done, let's consider \G's knowledge and uncertainty about the garbled evaluation.  There are three sources of \G's uncertainty:
\begin{itemize}
	\item True wire inputs.  This is the largest uncertainty (exponential number of possibilities in the number of inputs to the conditional), and the easiest to deal with.  \HK introduce a simple and elegant trick of the circuit simply setting to a fixed value, e.g., all zeros, all the inputs to the inactive branches.  This fully resolves this kind of uncertainty wrt \G.\todo{concisely clarify here what happens  when true branch is executed under a wrong guess, i.e. how we have 0 uncertainty.}
	\item True index of the actually taken branch, which we denote by \truth.
	\item \E's guess of value of \truth, which we denote by \guess.
\end{itemize}

Thus, there are $b^2$ pairs of labels (including  $b(b-1)$ garbage pairs of labels)  that the evaluator
could arrive at: one for each combination of (\truth,\guess), the  actually taken  and
the incorrectly guessed branches.
%


To build the multiplexer for garbage collection, the \HK generator assumes all-zero inputs for non-taken branches, and emulates ``in its head'' \E's evaluation under all possible (\truth,\guess) combinations.  
% must construct all $b(b-1)$ sets of garbage labels,
This requires \G to evaluate $b(b-1)$ circuits using garbage material.
This is the source of the $O(b^2)$ computation.


\subsection{Top-level Intuition of Our Approach}

In this work, we drive the added computation down from $O(b^2)$ to
$O(b \log b)$.
At a high level, our approach builds on the following key idea.  We reduce \G's uncertainty regarding \E's garbled evaluation and hence achieve $O(b\log b)$ cost:

\medskip
 
   Recall from discussion in~\Cref{sec:bsquaredcost} the  sources of \G's uncertainty, which results in $b^2$ garblings inside emulation of \E: $b$ possible options for each \truth and \guess ($\truth \in\{0,b-1\}, \guess\in\{0,b-1\})$.
   Once the pair  (\truth,\guess) are fixed, \G has fully deterministic view of the evaluation of the garbage (\truth,\guess)  combinations inside conditional.
   
   Our idea  is to reduce the number of \truth choices in this approach {\em wrt  \guess}. 
  
  Wlog, consider a balanced binary tree with $b$ branches as leaves. 
  For each leaf $b$, define $b$-th {\em sibling subtree at level $i$} to be the subtree rooted in a sibling of $i$-th node on the path to $b$. Thus, each branch has $\lceil \log b \rceil$ sibling subtrees.
  
  Consider at each level of the tree the dichotomy of whether \guess and \truth are in the same subtree. Our idea is to, given fixed \guess, enforce that \E identically processes \guess for all values of \truth which belong to the same sibling subtree of guess. This narrows \Gâ€™s uncertainty for each value of \guess: each value of \guess has only $\log b$ possible \truth options, one for each sibling subtree of \guess.
    
\medskip


\subsection{Next Sect}


the following is to move to technical intuition maybe.
\begin{itemize}

      \item The conditional branches are \emph{nested}.
    That is, instead of organizing $b$ branches as a vector, we
    organize them into a binary tree.
    Suppose \E guesses that a particular branch $\cir_i$ is taken,
    while in fact $\cir_j$, a member of a different subtree, is taken.
    The key idea of our approach is that we ensure that when \ev
    garbles the entire subtree containg $\cir_j$ but not $\cir_i$, she
    computes the same material, regardless of $j$.\todo{intuition still unclear}
  \item The multiplexer is \emph{not stacked}.
    The careful reader familiar with \cite{EPRINT:HeaKol20b} may be
    confused about our key idea: Stacked Garbling first presents a
    recursive binary tree approach to branching that they later
    discard in favor of a more efficient vector approach.
    So why is our binary tree approach better?
    The problem with \cite{EPRINT:HeaKol20b}'s recursive construction
    is that the evaluator recursively computes the multiplexer for
    nested sub-conditionals.
    However, doing so leads to a recursive emulation whereby \E
    emulates herself (and hence \G emulates himself as well).
    This recursion leads to quadratic cost for both players.
    The way out is to treat the multiplexer separately, and to opt not
    to stack it.
    If multiplexers are not stacked, then \E need not compute them.
\end{itemize}


% At a high level, the circuit generator, i.e. the player who constructs
% the material,
